{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a3fbbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.4.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.3.3-cp312-cp312-macosx_11_0_arm64.whl (10.7 MB)\n",
      "Downloading numpy-2.4.0-cp312-cp312-macosx_14_0_arm64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [pandas]2m3/4\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed numpy-2.4.0 pandas-2.3.3 pytz-2025.2 tzdata-2025.3\n"
     ]
    }
   ],
   "source": [
    "#! pip install https://github.com/kpu/kenlm/archive/master.zip\n",
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b18828a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import kenlm\n",
    "import subprocess\n",
    "import os\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import urllib.request\n",
    "import gzip\n",
    "import bz2\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01309d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "MODELS_DIR = Path(\"models\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a0e7ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 utterances\n",
      "\n",
      "First example:\n",
      "Reference: Вайлдер — Ортіс: відео нокауту\n",
      "Candidates (8):\n",
      "  1. Вайлдер — Ортіс: відео нокауту\n",
      "  2. Вайлдер — Ортіз: видео нокауту\n",
      "  3. Вайлдер — Ортіс: видео нокоуту\n"
     ]
    }
   ],
   "source": [
    "jsonl_file = \"ua_asr_hypotheses_500.jsonl\"\n",
    "\n",
    "data = []\n",
    "with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(data)} utterances\")\n",
    "print(f\"\\nFirst example:\")\n",
    "print(f\"Reference: {data[0]['reference']}\")\n",
    "print(f\"Candidates ({len(data[0]['hypotheses'])}):\")\n",
    "for i, hyp in enumerate(data[0]['hypotheses'][:3], 1):\n",
    "    print(f\"  {i}. {hyp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a23ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPORA = {\n",
    "    'social': {\n",
    "        'url': 'https://lang.org.ua/static/downloads/ubertext2.0/social/sentenced/ubertext.social.filter_rus_gcld+short.text_only.txt.bz2',\n",
    "        'size': '87 MB'\n",
    "    },\n",
    "    'fiction': {\n",
    "        'url': 'https://lang.org.ua/static/downloads/ubertext2.0/fiction/sentenced/ubertext.fiction.filter_rus_gcld+short.text_only.txt.bz2',\n",
    "        'size': '398 MB'\n",
    "    },\n",
    "    'news': {\n",
    "        'url': 'https://lang.org.ua/static/downloads/ubertext2.0/news/sentenced/ubertext.news.filter_rus_gcld+short.text_only.txt.bz2',\n",
    "        'size': '3.4 GB'\n",
    "    }\n",
    "}\n",
    "\n",
    "def download_and_preprocess_corpus(name, url):\n",
    "    bz2_file = DATA_DIR / f\"{name}.txt.bz2\"\n",
    "    raw_file = DATA_DIR / f\"{name}_raw.txt\"\n",
    "    preprocessed_file = DATA_DIR / f\"{name}_preprocessed.txt\"\n",
    "    \n",
    "    if not preprocessed_file.exists():\n",
    "        if not raw_file.exists():\n",
    "            urllib.request.urlretrieve(url, bz2_file)\n",
    "            \n",
    "            with bz2.open(bz2_file, 'rt', encoding='utf-8') as f_in:\n",
    "                with open(raw_file, 'w', encoding='utf-8') as f_out:\n",
    "                    f_out.write(f_in.read())\n",
    "\n",
    "        with open(raw_file, 'r', encoding='utf-8') as f_in:\n",
    "            with open(preprocessed_file, 'w', encoding='utf-8') as f_out:\n",
    "                for i, line in enumerate(f_in):\n",
    "                    cleaned = line.lower().strip()\n",
    "                    if cleaned:\n",
    "                        f_out.write(cleaned + '\\n')\n",
    "        \n",
    "        print(f\"Preprocessed corpus saved\")\n",
    "    else:\n",
    "        print(f\"Preprocessed corpus already exists\")\n",
    "    \n",
    "    with open(preprocessed_file, 'r', encoding='utf-8') as f:\n",
    "        lines = sum(1 for _ in f)\n",
    "    print(f\"Corpus size: {lines:,} lines\")\n",
    "    \n",
    "    return preprocessed_file\n",
    "\n",
    "corpus_files = {}\n",
    "\n",
    "# skip social\n",
    "corpus_files['social'] = DATA_DIR / \"corpus_preprocessed.txt\"\n",
    "\n",
    "corpus_files['fiction'] = download_and_preprocess_corpus('fiction', CORPORA['fiction']['url'])\n",
    "corpus_files['news'] = download_and_preprocess_corpus('news', CORPORA['news']['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12e1d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmplz_path = f\"/Users/mykhailopavliuk/nlp-improving-speech-recognition/nlp-improving-speech-recognition/kenlm/build/bin/lmplz\"\n",
    "build_binary_path = f\"/Users/mykhailopavliuk/nlp-improving-speech-recognition/nlp-improving-speech-recognition/kenlm/build/bin/build_binary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "438840c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1-gram: ukrainian_1gram.bin\n",
      "  2-gram: ukrainian_2gram.bin\n",
      "  3-gram: ukrainian_3gram.bin\n"
     ]
    }
   ],
   "source": [
    "def train_kenlm(input_file, output_file, ngram_order):\n",
    "    \n",
    "    cmd = [\n",
    "        lmplz_path,\n",
    "        \"-o\", str(ngram_order),\n",
    "        \"--text\", str(input_file),\n",
    "        \"--arpa\", str(output_file),\n",
    "        \"--discount_fallback\"\n",
    "    ]\n",
    "    \n",
    "    subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "    \n",
    "    # convert to binary\n",
    "    binary_file = output_file.with_suffix('.bin')\n",
    "    cmd = [build_binary_path, str(output_file), str(binary_file)]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        output_file.unlink()\n",
    "        return binary_file\n",
    "    else:\n",
    "        print(f\"Binary conversion failed, keeping ARPA\")\n",
    "        return output_file\n",
    "\n",
    "# using 2-gram with context size 1 as 1-gram\n",
    "models = {}\n",
    "\n",
    "arpa_file = MODELS_DIR / f\"ukrainian_1gram.arpa\"\n",
    "models[1] = train_kenlm(preprocessed_file, arpa_file, 2)\n",
    "\n",
    "# actual 2-gram and 3-gram\n",
    "for n in [2, 3]:\n",
    "    arpa_file = MODELS_DIR / f\"ukrainian_{n}gram.arpa\"\n",
    "    models[n] = train_kenlm(preprocessed_file, arpa_file, n)\n",
    "\n",
    "for n, path in models.items():\n",
    "    print(f\"  {n}-gram: {path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "978e7984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1-gram model\n",
      "Loaded 2-gram model\n",
      "Loaded 3-gram model\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_path):\n",
    "    return kenlm.Model(str(model_path))\n",
    "\n",
    "def sentence_logprob(model, sent):\n",
    "    return model.score(sent.lower(), bos=True, eos=True)\n",
    "\n",
    "def sentence_perplexity(model, sent, order=None):\n",
    "    words = sent.lower().split()\n",
    "    if len(words) == 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    # for 1-gram, score words independently\n",
    "    if order == 1:\n",
    "        logprob = sum(model.score(word, bos=False, eos=False) for word in words)\n",
    "    else:\n",
    "        logprob = model.score(sent.lower(), bos=True, eos=True)\n",
    "    \n",
    "    perplexity = 10 ** (-logprob / len(words))\n",
    "    return perplexity\n",
    "\n",
    "loaded_models = {}\n",
    "for n, path in models.items():\n",
    "    loaded_models[n] = load_model(path)\n",
    "    print(f\"Loaded {n}-gram model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19ce2af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: Вайлдер — Ортіс: відео нокауту\n",
      "\n",
      "1-gram model:\n",
      "Best candidate:  Вайлдер — Ортіс: відео нокауту\n",
      "Perplexity: 199754.37\n",
      "Worst candidate: Войлдер — Ортіз: вітео нокауту\n",
      "Perplexity: 1347445.37\n",
      "\n",
      "2-gram model:\n",
      "Best candidate:  Вайлдер — Ортіс: відео нокауту\n",
      "Perplexity: 448904.31\n",
      "Worst candidate: Войлдер — Ортіз: вітео нокауту\n",
      "Perplexity: 6729788.32\n",
      "\n",
      "3-gram model:\n",
      "Best candidate:  Вайлдер — Ортіс: відео нокауту\n",
      "Perplexity: 404277.82\n",
      "Worst candidate: Войлдер — Ортіз: вітео нокауту\n",
      "Perplexity: 6281724.98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def rerank_hypotheses(hypotheses, model, order=None):\n",
    "    scored = []\n",
    "    for hyp in hypotheses:\n",
    "        ppl = sentence_perplexity(model, hyp, order=order)\n",
    "        scored.append((hyp, ppl))\n",
    "    \n",
    "    # sort by perplexity (ascending)\n",
    "    scored.sort(key=lambda x: x[1])\n",
    "    return scored\n",
    "\n",
    "example = data[0]\n",
    "print(f\"Reference: {example['reference']}\\n\")\n",
    "\n",
    "for n in [1, 2, 3]:\n",
    "    print(f\"{n}-gram model:\")\n",
    "    ranked = rerank_hypotheses(example['hypotheses'], loaded_models[n], order=n)\n",
    "    \n",
    "    print(f\"Best candidate:  {ranked[0][0]}\")\n",
    "    print(f\"Perplexity: {ranked[0][1]:.2f}\")\n",
    "    \n",
    "    print(f\"Worst candidate: {ranked[-1][0]}\")\n",
    "    print(f\"Perplexity: {ranked[-1][1]:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a0839e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 1-gram model\n",
      "Accuracy: 60.80% (304/500)\n",
      "\n",
      "Evaluating 2-gram model\n",
      "Accuracy: 73.60% (368/500)\n",
      "\n",
      "Evaluating 3-gram model\n",
      "Accuracy: 76.00% (380/500)\n",
      "\n",
      "\n",
      "Summary:\n",
      "1-gram: 60.80%\n",
      "2-gram: 73.60%\n",
      "3-gram: 76.00%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(data, model, order=None):\n",
    "    correct = 0\n",
    "    total = len(data)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for item in data:\n",
    "        reference = item['reference']\n",
    "        hypotheses = item['hypotheses']\n",
    "\n",
    "        ranked = rerank_hypotheses(hypotheses, model, order=order)\n",
    "        best_candidate = ranked[0][0]\n",
    "        \n",
    "        is_correct = (best_candidate == reference)\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        \n",
    "        results.append({\n",
    "            'utt_id': item['utt_id'],\n",
    "            'reference': reference,\n",
    "            'predicted': best_candidate,\n",
    "            'correct': is_correct,\n",
    "            'perplexity': ranked[0][1]\n",
    "        })\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy, results\n",
    "\n",
    "\n",
    "all_results = {}\n",
    "for n in [1, 2, 3]:\n",
    "    print(f\"Evaluating {n}-gram model\")\n",
    "    accuracy, results = evaluate_model(data, loaded_models[n], order=n)\n",
    "    all_results[n] = {'accuracy': accuracy, 'results': results}\n",
    "    print(f\"Accuracy: {accuracy:.2%} ({int(accuracy * len(data))}/{len(data)})\\n\")\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "for n in [1, 2, 3]:\n",
    "    acc = all_results[n]['accuracy']\n",
    "    print(f\"{n}-gram: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c40ac953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fiction corpus\n",
      "Loaded 1-gram model\n",
      "Loaded 2-gram model\n",
      "Loaded 3-gram model\n",
      "\n",
      "Evaluating on fiction\n",
      "  1-gram: 55.40%\n",
      "  2-gram: 61.20%\n",
      "  3-gram: 63.40%\n",
      "Training on news corpus\n",
      "Loaded 1-gram model\n",
      "Loaded 2-gram model\n",
      "Loaded 3-gram model\n",
      "\n",
      "Evaluating on news\n",
      "  1-gram: 60.00%\n",
      "  2-gram: 79.60%\n",
      "  3-gram: 80.60%\n"
     ]
    }
   ],
   "source": [
    "all_corpus_results = {\n",
    "    'social': all_results\n",
    "}\n",
    "\n",
    "for corpus_name in ['fiction', 'news']:\n",
    "    print(f\"Training on {corpus_name} corpus\")\n",
    "\n",
    "    models = {}\n",
    "    for n in [1, 2, 3]:\n",
    "        arpa_file = MODELS_DIR / f\"{corpus_name}_{n}gram.arpa\"\n",
    "        \n",
    "        if n == 1:\n",
    "            model_file = train_kenlm(corpus_files[corpus_name], arpa_file, 2)\n",
    "        else:\n",
    "            model_file = train_kenlm(corpus_files[corpus_name], arpa_file, n)\n",
    "        \n",
    "        models[n] = model_file\n",
    "    \n",
    "    loaded_models = {}\n",
    "    for n, path in models.items():\n",
    "        loaded_models[n] = kenlm.Model(str(path))\n",
    "        print(f\"Loaded {n}-gram model\")\n",
    "    \n",
    "    print(f\"\\nEvaluating on {corpus_name}\")\n",
    "    corpus_results = {}\n",
    "    for n in [1, 2, 3]:\n",
    "        accuracy, results = evaluate_model(data, loaded_models[n], order=n)\n",
    "        corpus_results[n] = {'accuracy': accuracy, 'results': results}\n",
    "        print(f\"  {n}-gram: {accuracy:.2%}\")\n",
    "    \n",
    "    all_corpus_results[corpus_name] = corpus_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "58c482f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Corpus N-gram Accuracy Correct\n",
      " Social 1-gram   60.80% 304/500\n",
      " Social 2-gram   73.60% 368/500\n",
      " Social 3-gram   76.00% 380/500\n",
      "Fiction 1-gram   55.40% 277/500\n",
      "Fiction 2-gram   61.20% 306/500\n",
      "Fiction 3-gram   63.40% 317/500\n",
      "   News 1-gram   60.00% 300/500\n",
      "   News 2-gram   79.60% 398/500\n",
      "   News 3-gram   80.60% 403/500\n"
     ]
    }
   ],
   "source": [
    "results_table = []\n",
    "for corpus in ['social', 'fiction', 'news']:\n",
    "    for n in [1, 2, 3]:\n",
    "        acc = all_corpus_results[corpus][n]['accuracy']\n",
    "        results_table.append({\n",
    "            'Corpus': corpus.capitalize(),\n",
    "            'N-gram': f\"{n}-gram\",\n",
    "            'Accuracy': f\"{acc:.2%}\",\n",
    "            'Correct': f\"{int(acc * len(data))}/{len(data)}\"\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(results_table)\n",
    "print(df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
