{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a3fbbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
      "  Downloading https://github.com/kpu/kenlm/archive/master.zip\n",
      "\u001b[2K     \u001b[32m\\\u001b[0m \u001b[32m553.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: kenlm\n",
      "  Building wheel for kenlm (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kenlm: filename=kenlm-0.2.0-cp312-cp312-macosx_14_0_arm64.whl size=498409 sha256=2a10934b044cf31341c94fb4e25b6a2de3c3ca54d9870f175b175422348bf45d\n",
      "  Stored in directory: /private/var/folders/5c/m8ryn9cx68304n_x62p_9xzh0000gn/T/pip-ephem-wheel-cache-q5h9i99x/wheels/92/c8/12/56d187154e078f0eaa74d059017fc1afe1c4d91fbce02ce8d9\n",
      "Successfully built kenlm\n",
      "Installing collected packages: kenlm\n",
      "Successfully installed kenlm-0.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install https://github.com/kpu/kenlm/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e51b862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updating Homebrew...\u001b[0m\n",
      "Adjust how often this is run with `$HOMEBREW_AUTO_UPDATE_SECS` or disable with\n",
      "`$HOMEBREW_NO_AUTO_UPDATE=1`. Hide these hints with `$HOMEBREW_NO_ENV_HINTS=1` (see `man brew`).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/portable-ruby/blobs/sha256:1c98fa49eacc935640a6f8e10a2bf33f14cfc276804b71ddb658ea45ba99d167\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring portable-ruby-3.4.8.arm64_big_sur.bottle.tar.gz\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updated Homebrew!\u001b[0m\n",
      "Updated 4 taps (hashicorp/tap, txn2/tap, homebrew/core and homebrew/cask).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Formulae\u001b[0m\n",
      "adplay: Command-line player for OPL2 music\n",
      "astra: Command-Line Interface for DataStax Astra\n",
      "bookokrat: Terminal EPUB Book Reader\n",
      "bumpp: Interactive CLI that bumps your version numbers and more\n",
      "calm-cli: CLI allows you to interact with the Common Architecture Language Model (CALM)\n",
      "carl: Calendar for the command-line\n",
      "cinecli: Browse, inspect, and launch movie torrents directly from your terminal\n",
      "ctre: Compile-time PCRE-compatible regular expression matcher for C++\n",
      "custom-install: Install CIA files directly to Nintendo 3DS SD card\n",
      "depot: Build your Docker images in the cloud\n",
      "dnspyre: CLI tool for a high QPS DNS benchmark\n",
      "docker-language-server: Language server for Dockerfiles, Compose files, and Bake files\n",
      "dotnet@9: .NET Core\n",
      "flecs: Fast entity component system for C & C++\n",
      "fresh-editor: Text editor for your terminal: easy, powerful and fast\n",
      "garage: S3 object store so reliable you can run it outside datacenters\n",
      "git-get: Better way to clone, organize and manage multiple git repositories\n",
      "git-pages: Scalable static site server for Git forges\n",
      "git-pages-cli: Tool for publishing a site to a git-pages server\n",
      "global-arrays: Partitioned Global Address Space (PGAS) library for distributed arrays\n",
      "goat: General purpose AT Protocol CLI in Go\n",
      "gtl: Greg's Template Library of useful classes\n",
      "gup: Update binaries installed by go install\n",
      "hayagriva: Bibliography management tool\n",
      "headson: Head/tail for structured data\n",
      "jsonfmt: Like gofmt, but for JSON files\n",
      "khaos: Kafka traffic simulator for observability and chaos engineering\n",
      "klog: Command-line tool for time tracking in a human-readable, plain-text file format\n",
      "kubernetes-cli@1.34: Kubernetes command-line interface\n",
      "kyua: Testing framework for infrastructure software\n",
      "labctl: CLI tool for interacting with iximiuz labs and playgrounds\n",
      "libevdev: Wrapper library for evdev devices\n",
      "lispkit: Scheme framework for extension and scripting languages on macOS and iOS\n",
      "macchanger: Change your mac address, for macOS\n",
      "mapscii: Whole World In Your Console\n",
      "mcp-scan: Constrain, log and scan your MCP connections for security vulnerabilities\n",
      "mistral-vibe: Minimal CLI coding agent\n",
      "mole: Deep clean and optimize your Mac\n",
      "mq: Jq-like command-line tool for markdown processing\n",
      "mufetch: Neofetch-style music cli\n",
      "neo4j-mcp: Neo4j official Model Context Protocol server for AI tools\n",
      "netshow: Interactive network connection monitor with friendly service names\n",
      "nkt: TUI for fast and simple interacting with your BibLaTeX database\n",
      "octodns: Tools for managing DNS across multiple providers\n",
      "papeer: Convert websites into eBooks and Markdown\n",
      "papis: Powerful command-line document and bibliography manager\n",
      "patchpal: AI Assisted Patch Backporting Tool Frontend\n",
      "phantom: CLI tool for seamless parallel development with Git worktrees\n",
      "pixlet: App runtime and UX toolkit for pixel-based apps\n",
      "pony-language-server: Language server for Pony\n",
      "pvetui: Terminal UI for Proxmox VE\n",
      "rad: Modern CLI scripts made easy\n",
      "redu: Ncdu for your restic repository\n",
      "rmrfrs: Filesystem cleaning tool\n",
      "rockcraft: Tool to create OCI images using the language from Snapcraft and Charmcraft\n",
      "ruby@3.4: Powerful, clean, object-oriented scripting language\n",
      "save3ds_fuse: Extract/Import/FUSE for 3DS save/extdata/database\n",
      "slicot: Fortran subroutines library for systems and control\n",
      "snitch: Prettier way to inspect network connections\n",
      "superseedr: BitTorrent Client in your Terminal\n",
      "svt-vp9: Scalable Video Technology for VP9 Encoder\n",
      "svu: Semantic version utility\n",
      "talm: Manage Talos Linux configurations the GitOps way\n",
      "termshot: Creates screenshots based on terminal command output\n",
      "tfclean: Remove applied moved block, import block, etc\n",
      "topydo: Todo list application using the todo.txt format\n",
      "tree-sitter@0.25: Incremental parsing library\n",
      "tronbyt-server: Manage your apps on your Tronbyt (flashed Tidbyt) completely locally\n",
      "ty: Extremely fast Python type checker, written in Rust\n",
      "vacuum: World's fastest OpenAPI & Swagger linter\n",
      "wasm-bindgen: Facilitating high-level interactions between Wasm modules and JavaScript\n",
      "wifitui: Fast featureful friendly wifi terminal UI\n",
      "witr: Why is this running?\n",
      "wuchale: Protobuf-like i18n from plain code\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Casks\u001b[0m\n",
      "8bitdo-ultimate-software-v2: Control every piece of your controller\n",
      "alma: AI chat application\n",
      "comet: Web browser with integrated AI assistant\n",
      "conar: AI-powered database and data management tool\n",
      "copilot-cli@prerelease: Brings the power of Copilot coding agent directly to your terminal\n",
      "crypto-native-app-ng: Encrypts and signs data on your computer and communicates with browser extension\n",
      "datadog-security-cli: Datadog Security Product CLI\n",
      "digiexam: Academic testing platform with device lockdown\n",
      "dnclient: Peer-to-peer VPN client for managed nebula networks\n",
      "elgato-studio: Capture and manage Elgato devices for content creation\n",
      "excire-search: Lightroom Classic plugin with automatic keywording and advanced search\n",
      "font-cause\n",
      "font-guguru-sans-code\n",
      "font-guguru-sans-code-nf\n",
      "glkvm: App for controlling GL.iNet KVM devices\n",
      "gonhanh: Vietnamese input method engine\n",
      "kubeterm: Kubernetes graphical management tool\n",
      "m32-edit: Remote control for Midas M32 audio consoles\n",
      "macdown-3000: Markdown editor with live preview and syntax highlighting\n",
      "mace: Simplify compliance baseline creation, auditing, and management\n",
      "maestro: AI agent command center\n",
      "maru-jan: Play japanese mahjong online\n",
      "monocle-app: Window dimming utility\n",
      "mpluginmanager: Installer for MeldaProduction audio plugins\n",
      "mstystudio: AI platform with local and online models\n",
      "nani: AI-powered translator\n",
      "novation-components: Manager and updater for Novation hardware\n",
      "opencode-desktop: AI coding agent desktop client\n",
      "oracle-data-modeler: Graphical tool for data modeling tasks\n",
      "papercut-mobility-print-client: Client for printing to PaperCut Mobility Print queues\n",
      "portalbox: Share a region of your screen in video calls\n",
      "smartsheet: Spreadsheet-style project management solution\n",
      "snapmaker-orca: Slicing software for Snapmaker 3D printers, a fork of OrcaSlicer\n",
      "sourcegit: Git GUI client\n",
      "support: Menu bar app for user and help desk support\n",
      "swiftdialog: Admin utility that presents custom dialogs or messages from shell scripts\n",
      "taphouse: Native GUI for Homebrew package management\n",
      "kubefwd\n",
      "typeless: AI voice dictation that turns speech into polished text\n",
      "uuremote: NetEase UU remote desktop access and control tool\n",
      "vcamapp: Face-tracking virtual avatar app\n",
      "visualdiffer: Visually compare folders and files\n",
      "wailbrew: Manage Homebrew packages with a UI\n",
      "wireless-workbench: Desktop app for RF coordination and wireless system management\n",
      "yingfu-online: Education app for teens\n",
      "yoink: Drag and drop utility\n",
      "\n",
      "You have \u001b[1m40\u001b[0m outdated formulae and \u001b[1m1\u001b[0m outdated cask installed.\n",
      "\n",
      "\u001b[33mWarning:\u001b[0m No available formula with the name \"kenlm\".\n",
      "\u001b[34m==>\u001b[0m \u001b[1mSearching for similarly named formulae and casks...\u001b[0m\n",
      "\u001b[31mError:\u001b[0m No formulae or casks found for kenlm.\n"
     ]
    }
   ],
   "source": [
    "! brew install kenlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b18828a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import kenlm\n",
    "import subprocess\n",
    "import os\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import urllib.request\n",
    "import gzip\n",
    "import bz2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01309d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "MODELS_DIR = Path(\"models\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a0e7ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 utterances\n",
      "\n",
      "First example:\n",
      "Reference: Вайлдер — Ортіс: відео нокауту\n",
      "Candidates (8):\n",
      "  1. Вайлдер — Ортіс: відео нокауту\n",
      "  2. Вайлдер — Ортіз: видео нокауту\n",
      "  3. Вайлдер — Ортіс: видео нокоуту\n"
     ]
    }
   ],
   "source": [
    "jsonl_file = \"ua_asr_hypotheses_500.jsonl\"\n",
    "\n",
    "data = []\n",
    "with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(data)} utterances\")\n",
    "print(f\"\\nFirst example:\")\n",
    "print(f\"Reference: {data[0]['reference']}\")\n",
    "print(f\"Candidates ({len(data[0]['hypotheses'])}):\")\n",
    "for i, hyp in enumerate(data[0]['hypotheses'][:3], 1):\n",
    "    print(f\"  {i}. {hyp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675447e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "554a23ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus saved to data/corpus.txt\n",
      "Corpus has 4,494,263 lines\n"
     ]
    }
   ],
   "source": [
    "# using social corpus for the first attempt\n",
    "corpus_url = \"https://lang.org.ua/static/downloads/ubertext2.0/social/sentenced/ubertext.social.filter_rus_gcld+short.text_only.txt.bz2\"\n",
    "corpus_file = DATA_DIR / \"social.txt.bz2\"\n",
    "corpus_txt = DATA_DIR / \"corpus.txt\"\n",
    "\n",
    "if not corpus_txt.exists():\n",
    "    urllib.request.urlretrieve(corpus_url, corpus_file)\n",
    "\n",
    "    with bz2.open(corpus_file, 'rt', encoding='utf-8') as f_in:\n",
    "        with open(corpus_txt, 'w', encoding='utf-8') as f_out:\n",
    "            f_out.write(f_in.read())\n",
    "    \n",
    "    print(f\"Corpus saved to {corpus_txt}\")\n",
    "else:\n",
    "    print(f\"Corpus already exists at {corpus_txt}\")\n",
    "\n",
    "with open(corpus_txt, 'r', encoding='utf-8') as f:\n",
    "    lines = sum(1 for _ in f)\n",
    "    \n",
    "print(f\"Corpus has {lines:,} lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8246b2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100,000 lines...\n",
      "  Processed 200,000 lines...\n",
      "  Processed 300,000 lines...\n",
      "  Processed 400,000 lines...\n",
      "  Processed 500,000 lines...\n",
      "  Processed 600,000 lines...\n",
      "  Processed 700,000 lines...\n",
      "  Processed 800,000 lines...\n",
      "  Processed 900,000 lines...\n",
      "  Processed 1,000,000 lines...\n",
      "  Processed 1,100,000 lines...\n",
      "  Processed 1,200,000 lines...\n",
      "  Processed 1,300,000 lines...\n",
      "  Processed 1,400,000 lines...\n",
      "  Processed 1,500,000 lines...\n",
      "  Processed 1,600,000 lines...\n",
      "  Processed 1,700,000 lines...\n",
      "  Processed 1,800,000 lines...\n",
      "  Processed 1,900,000 lines...\n",
      "  Processed 2,000,000 lines...\n",
      "  Processed 2,100,000 lines...\n",
      "  Processed 2,200,000 lines...\n",
      "  Processed 2,300,000 lines...\n",
      "  Processed 2,400,000 lines...\n",
      "  Processed 2,500,000 lines...\n",
      "  Processed 2,600,000 lines...\n",
      "  Processed 2,700,000 lines...\n",
      "  Processed 2,800,000 lines...\n",
      "  Processed 2,900,000 lines...\n",
      "  Processed 3,000,000 lines...\n",
      "  Processed 3,100,000 lines...\n",
      "  Processed 3,200,000 lines...\n",
      "  Processed 3,300,000 lines...\n",
      "  Processed 3,400,000 lines...\n",
      "  Processed 3,500,000 lines...\n",
      "  Processed 3,600,000 lines...\n",
      "  Processed 3,700,000 lines...\n",
      "  Processed 3,800,000 lines...\n",
      "  Processed 3,900,000 lines...\n",
      "  Processed 4,000,000 lines...\n",
      "  Processed 4,100,000 lines...\n",
      "  Processed 4,200,000 lines...\n",
      "  Processed 4,300,000 lines...\n",
      "  Processed 4,400,000 lines...\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    return text.lower().strip()\n",
    "\n",
    "preprocessed_file = DATA_DIR / \"corpus_preprocessed.txt\"\n",
    "\n",
    "with open(corpus_txt, 'r', encoding='utf-8') as f_in:\n",
    "    with open(preprocessed_file, 'w', encoding='utf-8') as f_out:\n",
    "        for i, line in enumerate(f_in):\n",
    "            cleaned = preprocess_text(line)\n",
    "            if cleaned:  # skip empty lines\n",
    "                f_out.write(cleaned + '\\n')\n",
    "            \n",
    "            if (i + 1) % 100000 == 0:\n",
    "                print(f\"  Processed {i+1:,} lines...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12e1d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmplz_path = f\"/Users/mykhailopavliuk/nlp-improving-speech-recognition/nlp-improving-speech-recognition/kenlm/build/bin/lmplz\"\n",
    "build_binary_path = f\"/Users/mykhailopavliuk/nlp-improving-speech-recognition/nlp-improving-speech-recognition/kenlm/build/bin/build_binary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "438840c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-gram ARPA model created\n",
      "Binary model saved to models/ukrainian_2gram.bin\n",
      "3-gram ARPA model created\n",
      "Binary model saved to models/ukrainian_3gram.bin\n",
      "  2-gram: models/ukrainian_2gram.bin\n",
      "  3-gram: models/ukrainian_3gram.bin\n"
     ]
    }
   ],
   "source": [
    "def train_kenlm(input_file, output_file, ngram_order):\n",
    "    cmd = [\n",
    "        lmplz_path,\n",
    "        \"-o\", str(ngram_order),\n",
    "        \"--text\", str(input_file),\n",
    "        \"--arpa\", str(output_file),\n",
    "        \"--discount_fallback\"\n",
    "    ]\n",
    "    \n",
    "    subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "    print(f\"{ngram_order}-gram ARPA model created\")\n",
    "    \n",
    "    # binary for faster loading\n",
    "    binary_file = output_file.with_suffix('.bin')\n",
    "    cmd = [build_binary_path, str(output_file), str(binary_file)]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(f\"Binary conversion failed: {result.stderr}\")\n",
    "        return output_file\n",
    "    else:\n",
    "        print(f\"Binary model saved to {binary_file}\")\n",
    "        return binary_file\n",
    "\n",
    "models = {}\n",
    "for n in [2, 3]:\n",
    "    arpa_file = MODELS_DIR / f\"ukrainian_{n}gram.arpa\"\n",
    "    model_file = train_kenlm(preprocessed_file, arpa_file, n)\n",
    "    models[n] = model_file\n",
    "\n",
    "for n, path in models.items():\n",
    "    print(f\"  {n}-gram: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "978e7984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2-gram model\n",
      "Loaded 3-gram model\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_path):\n",
    "    return kenlm.Model(str(model_path))\n",
    "\n",
    "def sentence_logprob(model, sent):\n",
    "    return model.score(sent.lower(), bos=True, eos=True)\n",
    "\n",
    "def sentence_perplexity(model, sent):\n",
    "    words = sent.lower().split()\n",
    "    if len(words) == 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    logprob = sentence_logprob(model, sent)\n",
    "    perplexity = 10 ** (-logprob / len(words))\n",
    "    return perplexity\n",
    "\n",
    "# 2-gram and 3-gram only\n",
    "loaded_models = {}\n",
    "for n, path in models.items():\n",
    "    loaded_models[n] = load_model(path)\n",
    "    print(f\"Loaded {n}-gram model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19ce2af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: Вайлдер — Ортіс: відео нокауту\n",
      "\n",
      "2-gram model:\n",
      "Best candidate:  Вайлдер — Ортіс: відео нокауту\n",
      "Perplexity: 448904.31\n",
      "Worst candidate: Войлдер — Ортіз: вітео нокауту\n",
      "Perplexity: 6729788.32\n",
      "\n",
      "3-gram model:\n",
      "Best candidate:  Вайлдер — Ортіс: відео нокауту\n",
      "Perplexity: 404277.82\n",
      "Worst candidate: Войлдер — Ортіз: вітео нокауту\n",
      "Perplexity: 6281724.98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def rerank_hypotheses(hypotheses, model):\n",
    "    scored = []\n",
    "    for hyp in hypotheses:\n",
    "        ppl = sentence_perplexity(model, hyp)\n",
    "        scored.append((hyp, ppl))\n",
    "    \n",
    "    # sort by perplexity (ascending)\n",
    "    scored.sort(key=lambda x: x[1])\n",
    "    return scored\n",
    "\n",
    "example = data[0]\n",
    "print(f\"Reference: {example['reference']}\\n\")\n",
    "\n",
    "for n in [2, 3]:\n",
    "    print(f\"{n}-gram model:\")\n",
    "    ranked = rerank_hypotheses(example['hypotheses'], loaded_models[n])\n",
    "    \n",
    "    print(f\"Best candidate:  {ranked[0][0]}\")\n",
    "    print(f\"Perplexity: {ranked[0][1]:.2f}\")\n",
    "    \n",
    "    print(f\"Worst candidate: {ranked[-1][0]}\")\n",
    "    print(f\"Perplexity: {ranked[-1][1]:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3f7936e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 2-gram model (Binary: ukrainian_2gram.bin)\n",
      "✓ Loaded 3-gram model (Binary: ukrainian_3gram.bin)\n"
     ]
    }
   ],
   "source": [
    "# Check which format we're using\n",
    "loaded_models = {}\n",
    "for n, path in models.items():\n",
    "    loaded_models[n] = load_model(path)\n",
    "    file_type = \"Binary\" if str(path).endswith('.bin') else \"ARPA\"\n",
    "    print(f\"✓ Loaded {n}-gram model ({file_type}: {path.name})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
